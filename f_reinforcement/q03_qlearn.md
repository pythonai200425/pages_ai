# Q-Learning â€” Rock ðŸŸ¡ Paper ðŸ“„ Scissors âœ‚ï¸

## 1ï¸âƒ£ What is Q-Learning?

Q-learning is a way of learning **while the game is still running**

Instead of waiting for the end, the agent:

* Makes a move
* Gets feedback
* Learns immediately
* Moves on

Think of it like this:

> â€œIâ€™ll adjust my thinking step by step as I goâ€

## 2ï¸âƒ£ Big idea: learning from guesses

Q-learning does **not** wait for the final result

It uses:

* What just happened (reward now)
* What it *thinks* will happen next (current knowledge)

So Q-learning learns from **estimates**, not certainty

Thatâ€™s the key difference from Monte Carlo

## 3ï¸âƒ£ Q-table (same memory, different usage)

Q-learning uses the **same Q-table** as Monte Carlo

The table still answers:

> â€œIf I am in this situation and take this action, how good is it?â€

The difference is:

* Monte Carlo writes after the game ends
* Q-learning writes **during the game**

## 4ï¸âƒ£ One-step update (very important)

In Q-learning:

* Only **ONE cell** is updated at a time
* The current (state, action)
* No going back over the whole path

Learning spreads backward **slowly**, over many games

## 5ï¸âƒ£ How a single update works (human version)

When the agent:

1. Is in a state
2. Chooses an action
3. Gets a reward
4. Moves to a new state

It thinks:

> â€œWas this better or worse than I expected, considering what usually happens next?â€

Then it nudges the current Q-table value slightly

## 6ï¸âƒ£ Example: learning during a game

Path:

A â†’ B â†’ C â†’ D â†’ WIN (+1)

Q-learning updates like this:

* At A â†’ update A
* At B â†’ update B
* At C â†’ update C
* At D â†’ update D

Each update uses:

* Reward now
* Best guess about the future

No waiting for the end

## 7ï¸âƒ£ Gamma (Î³) in Q-learning

Gamma controls **how much the future matters**

* Low gamma:

  * Focus on immediate reward
  * Short memory

* High gamma:

  * Future reward matters
  * Long memory

Gamma does **not** choose actions
It only affects how values are updated

## 8ï¸âƒ£ Epsilon (Îµ) in Q-learning

Epsilon controls **how actions are chosen**

* High epsilon:

  * Random actions
  * Exploration

* Low epsilon:

  * Choose best-known action
  * Exploitation

Epsilon does **not** affect learning math
It only affects behavior

## 9ï¸âƒ£ Why Q-learning works

Even though early guesses are bad:

* The agent keeps correcting itself
* Good paths get reinforced
* Bad paths fade away

Learning happens gradually, not instantly

## ðŸ”Ÿ When Q-learning is a good choice

Q-learning is great when:

* Games are long
* Rewards are delayed
* You want faster learning
* Waiting for the end is expensive

Q-learning is harder to understand
But very powerful

## ðŸ”’ Final brain-friendly summary ðŸ§ 

* Q-learning learns **during the game**
* It updates **one step at a time**
* It learns from **guesses + correction**
* Gamma = how much the future matters
* Epsilon = curiosity vs confidence

Q-learning is basically:

> Try â†’ Adjust â†’ Try again â†’ Adjust again â†’ Improve
