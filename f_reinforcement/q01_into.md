# Reinforcement Learning (RL) â€” Tic Tac Toe âŒ â­•

# Introduction

## 1ï¸âƒ£ What Reinforcement Learning even is?

Reinforcement Learning is a way to teach a computer **by experience**, not by rules

Think about a kid learning a game:

* The kid tries something
* The kid sees if it was good or bad
* The kid remembers what worked
* Next time, the kid tries to do better

That kid is called the **agent**

## 2ï¸âƒ£ Our game: Tic Tac Toe

The rules (simple life rules):

* 3x3 board
* Two players: âŒ and â­•
* First to align 3 in a row (horizontal, vertical, diagonal) wins
* If the board fills with no winner â†’ Draw

The agent plays against an opponent

## 3ï¸âƒ£ Who is the agent?

The **agent** is the learner

In our case:

* The agent chooses one empty square
* The environment updates the board and returns a result

Possible results:

* Win â†’ good ğŸ˜„
* Lose â†’ bad ğŸ˜¢
* Draw â†’ meh ğŸ˜
* Or game continues

## 4ï¸âƒ£ Rewards (how the agent feels)

We convert feelings into numbers (computers love numbers)

* Win  â†’ +1
* Draw â†’  0
* Lose â†’ -1
* Non-terminal move â†’ 0

This number is called the **reward**

<img src="ttt_game_flow.png" />

## 5ï¸âƒ£ What is a Q-table? (the heart of RL)

Q-table = the agent's **cheat sheet / memory**

It answers this question:

ğŸ‘‰ "If I am in this board state and I place my mark in this square, how good is it?"

For Tic Tac Toe, the table is larger than Rock Paper Scissors

States = board configurations
Actions = which empty square I choose

Example (conceptual, not code):

Board state S:

âŒ â­• âŒ  
â­• âŒ â¬œ  
â¬œ â­• â¬œ  

Possible actions:

* Place at (2,0) â†’ value 0.4
* Place at (2,2) â†’ value 0.9
* Place at (1,2) â†’ value 0.1

Higher number = better idea

Simple example (monte carlo score):

<img src="qtable_ttt3.png" />

## 6ï¸âƒ£ How the Q-table is updated - QLearning Score

After each move:

1. Agent picks an action
2. Game gives a reward (if terminal)
3. Agent updates ONE number in the Q-table

<img src="qtable_ttt5.png" />

Simple idea:

"Was this better or worse than I expected?"

Slow brain formula (no math panic):

new value = old value + small correction

That correction depends on reward and future hope

# Flow and Strategy

## 7ï¸âƒ£ Gamma (Î³) â€” thinking about the future

Gamma answers:

ğŸ‘‰ "Do I care about future rewards or only NOW?"

* Gamma close to 0

  * I only care about this move
  * Greedy mindset

* Gamma close to 1

  * I care about winning in the long run
  * Strategy brain ğŸ§ 

Typical value: **0.9**

## 8ï¸âƒ£ Epsilon (Îµ) â€” curiosity vs confidence

Epsilon answers:

ğŸ‘‰ "Should I try something random?"

* High epsilon (like 1.0)

  * Try random moves
  * Exploration phase ğŸ§ª

* Low epsilon (like 0.1)

  * Use what I already know
  * Exploitation phase ğŸ¯

Training usually looks like:

* Start with high epsilon
* Slowly reduce it

## 9ï¸âƒ£ Full learning loop (slow recap)

1. Agent looks at Q-table
2. Agent maybe explores (epsilon)
3. Agent chooses a square
4. Game returns reward (if terminal)
5. Q-table is updated
6. Score is updated
7. Repeat MANY games

## Summary ğŸ§ 

* Agent = learner
* Reward = feedback
* Q-table = memory
* Gamma = future thinking
* Epsilon = curiosity

Reinforcement Learning is literally:

> Try â†’ Fail â†’ Remember â†’ Improve
