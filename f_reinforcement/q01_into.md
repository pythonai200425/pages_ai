# Reinforcement Learning (RL) â€” Rock ðŸŸ¡ Paper ðŸ“„ Scissors âœ‚ï¸

## 1ï¸âƒ£ What Reinforcement Learning even is?

Reinforcement Learning is a way to teach a computer **by experience**, not by rules

Think about a kid learning a game:

* The kid tries something
* The kid sees if it was good or bad
* The kid remembers what worked
* Next time, the kid tries to do better

That kid is called the **agent**

## 2ï¸âƒ£ Our game: Rock Paper Scissors

The rules (simple life rules):

* ðŸŸ¡ Rock beats âœ‚ï¸ Scissors
* âœ‚ï¸ Scissors beats ðŸ“„ Paper
* ðŸ“„ Paper beats ðŸŸ¡ Rock

The agent plays against an opponent

## 3ï¸âƒ£ Who is the agent?

The **agent** is the learner

In our case:

* The agent chooses one move: ðŸŸ¡ / ðŸ“„ / âœ‚ï¸
* The environment answers with a result

Possible results:

* Win â†’ good ðŸ˜„
* Lose â†’ bad ðŸ˜¢
* Draw â†’ meh ðŸ˜

## 4ï¸âƒ£ Rewards (how the agent feels)

We convert feelings into numbers (computers love numbers)

* Win  â†’ +1
* Draw â†’  0
* Lose â†’ -1

This number is called the **reward**

## 5ï¸âƒ£ What is a Q-table? (the heart of RL)

Q-table = the agent's **cheat sheet / memory**

It answers this question:

ðŸ‘‰ "If I am in this situation and I do this action, how good is it?"

For Rock Paper Scissors, the table is tiny

States = what happened last round
Actions = what I choose now

Example (conceptual, not code):

* Last round was a WIN

  * If I play ðŸŸ¡ â†’ value 0.2
  * If I play ðŸ“„ â†’ value 0.5
  * If I play âœ‚ï¸ â†’ value 0.1

Higher number = better idea

## 6ï¸âƒ£ How the Q-table is updated (baby steps)

After each round:

1. Agent picks an action
2. Game gives a reward
3. Agent updates ONE number in the Q-table

Simple idea:

"Was this better or worse than I expected?"

Slow brain formula (no math panic):

new value = old value + small correction

That correction depends on reward and future hope

## 7ï¸âƒ£ Gamma (Î³) â€” thinking about the future

Gamma answers:

ðŸ‘‰ "Do I care about future rewards or only NOW?"

* Gamma close to 0

  * I only care about this round
  * YOLO mindset

* Gamma close to 1

  * I care about winning in the long run
  * Chess brain ðŸ§ 

Typical value: **0.9**

## 8ï¸âƒ£ Epsilon (Îµ) â€” curiosity vs confidence

Epsilon answers:

ðŸ‘‰ "Should I try something random?"

* High epsilon (like 1.0)

  * Try random moves
  * Exploration phase ðŸ§ª

* Low epsilon (like 0.1)

  * Use what I already know
  * Exploitation phase ðŸŽ¯

Training usually looks like:

* Start with high epsilon
* Slowly reduce it

## 9ï¸âƒ£ Showing the score

We keep a score counter:

* Wins: +1
* Losses: -1
* Draws: 0

Total score shows:

ðŸ‘‰ "Is the agent actually learning?"

If score goes up over many games â†’ ðŸŽ‰ success

## ðŸ”Ÿ Full learning loop (slow recap)

1. Agent looks at Q-table
2. Agent maybe explores (epsilon)
3. Agent chooses ðŸŸ¡ / ðŸ“„ / âœ‚ï¸
4. Game returns reward
5. Q-table is updated
6. Score is updated
7. Repeat MANY times

## Final brain-friendly summary ðŸ§ 

* Agent = learner
* Reward = feedback
* Q-table = memory
* Gamma = future thinking
* Epsilon = curiosity

Reinforcement Learning is literally:

> Try â†’ Fail â†’ Remember â†’ Improve

