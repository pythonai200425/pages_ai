# Reinforcement Learning (RL) ‚Äî Tic Tac Toe ‚ùå ‚≠ï

# Introduction

## 1Ô∏è‚É£ What Reinforcement Learning even is?

Reinforcement Learning is a way to teach a computer **by experience**, not by rules

Think about a kid learning a game:

* The kid tries something
* The kid sees if it was good or bad
* The kid remembers what worked
* Next time, the kid tries to do better

That kid is called the **agent**

## 2Ô∏è‚É£ Our game: Tic Tac Toe

The rules (simple life rules):

* 3x3 board
* Two players: ‚ùå and ‚≠ï
* First to align 3 in a row (horizontal, vertical, diagonal) wins
* If the board fills with no winner ‚Üí Draw

The agent plays against an opponent

## 3Ô∏è‚É£ Who is the agent?

The **agent** is the learner

In our case:

* The agent chooses one empty square
* The environment updates the board and returns a result

Possible results:

* Win ‚Üí good üòÑ
* Lose ‚Üí bad üò¢
* Draw ‚Üí meh üòê
* Or game continues

## 4Ô∏è‚É£ Rewards (how the agent feels)

We convert feelings into numbers (computers love numbers)

* Win  ‚Üí +1
* Draw ‚Üí  0
* Lose ‚Üí -1
* Non-terminal move ‚Üí 0

This number is called the **reward**

## 5Ô∏è‚É£ What is a Q-table? (the heart of RL)

Q-table = the agent's **cheat sheet / memory**

It answers this question:

üëâ "If I am in this board state and I place my mark in this square, how good is it?"

For Tic Tac Toe, the table is larger than Rock Paper Scissors

States = board configurations
Actions = which empty square I choose

Example (conceptual, not code):

Board state S:

‚ùå ‚≠ï ‚ùå
‚≠ï ‚ùå ‚¨ú
‚¨ú ‚≠ï ‚¨ú

Possible actions:

* Place at (2,0) ‚Üí value 0.4
* Place at (2,2) ‚Üí value 0.9
* Place at (1,2) ‚Üí value 0.1

Higher number = better idea

## 6Ô∏è‚É£ How the Q-table is updated (baby steps)

After each move:

1. Agent picks an action
2. Game gives a reward (if terminal)
3. Agent updates ONE number in the Q-table

Simple idea:

"Was this better or worse than I expected?"

Slow brain formula (no math panic):

new value = old value + small correction

That correction depends on reward and future hope

# Let's get technical

## 7Ô∏è‚É£ Gamma (Œ≥) ‚Äî thinking about the future

Gamma answers:

üëâ "Do I care about future rewards or only NOW?"

* Gamma close to 0

  * I only care about this move
  * Greedy mindset

* Gamma close to 1

  * I care about winning in the long run
  * Strategy brain üß†

Typical value: **0.9**

## 8Ô∏è‚É£ Epsilon (Œµ) ‚Äî curiosity vs confidence

Epsilon answers:

üëâ "Should I try something random?"

* High epsilon (like 1.0)

  * Try random moves
  * Exploration phase üß™

* Low epsilon (like 0.1)

  * Use what I already know
  * Exploitation phase üéØ

Training usually looks like:

* Start with high epsilon
* Slowly reduce it

## 9Ô∏è‚É£ Showing the score

We keep a score counter:

* Wins: +1
* Losses: -1
* Draws: 0

Total score shows:

üëâ "Is the agent actually learning?"

If score goes up over many games ‚Üí üéâ success

## üîü Full learning loop (slow recap)

1. Agent looks at Q-table
2. Agent maybe explores (epsilon)
3. Agent chooses a square
4. Game returns reward (if terminal)
5. Q-table is updated
6. Score is updated
7. Repeat MANY games

## Final brain-friendly summary üß†

* Agent = learner
* Reward = feedback
* Q-table = memory
* Gamma = future thinking
* Epsilon = curiosity

Reinforcement Learning is literally:

> Try ‚Üí Fail ‚Üí Remember ‚Üí Improve

# Appendix A

## üß† Gamma vs Epsilon ‚Äî why we need BOTH

### üîë One-line difference (lock this in)

* **Gamma (Œ≥)** decides **how rewards are judged** (learning)
* **Epsilon (Œµ)** decides **how actions are chosen** (behavior)

They never do the same job

## 1Ô∏è‚É£ Gamma (Œ≥): how the agent THINKS about rewards

Gamma answers:

üëâ "When I update my memory‚Ä¶ do I care about the future?"

* **Low gamma**

  * Only immediate reward matters
  * "Did this move win now?"

* **High gamma**

  * Immediate reward + future rewards
  * "Does this position lead to winning later?"

‚ö†Ô∏è Gamma does **NOT** choose actions

It only affects how numbers are written into the Q-table

## 2Ô∏è‚É£ Epsilon (Œµ): how the agent ACTS

Epsilon answers:

üëâ "Do I trust my knowledge or act randomly?"

* **High epsilon**

  * Ignore the Q-table
  * Try random empty squares

* **Low epsilon**

  * Follow the highest Q-value
  * Play the best-known move

‚ö†Ô∏è Epsilon does **NOT** affect learning math

It only affects what action is picked

## 3Ô∏è‚É£ The missing mental model (VERY important)

Every move has TWO separate steps:

1. **Choose an action** ‚Üí controlled by **epsilon**
2. **Learn from the result** ‚Üí controlled by **gamma**

They happen at different times

## 4Ô∏è‚É£ Why low gamma does NOT replace epsilon

Key misunderstanding:

> "Low gamma means short thinking, so I don't need exploration"

Because:

* Gamma does not choose actions
* Without epsilon, the agent NEVER explores

Example:

* Agent wins once by playing center first
* Q-table prefers center
* Without epsilon ‚Üí agent always picks center
* Never learns better long-term patterns

## 5Ô∏è‚É£ What breaks if one is missing

### ‚ùå No epsilon

* No exploration
* Gets stuck in habits
* Learns slowly or incorrectly

### ‚ùå No gamma

* No long-term planning
* Strategy never stabilizes

## üîí Final lock-in summary

* **Gamma = how far into the future learning looks**
* **Epsilon = whether the agent explores or exploits**
* Low gamma ‚â† safe choices
* Low epsilon ‚â† short-term thinking

Both are required for learning to actually work
