# Reinforcement Learning (RL) â€” Tic Tac Toe âŒ â­•

# Introduction

## 1ï¸âƒ£ What Reinforcement Learning even is?

Reinforcement Learning is a way to teach a computer **by experience**, not by rules

Think about a kid learning a game:

* The kid tries something
* The kid sees if it was good or bad
* The kid remembers what worked
* Next time, the kid tries to do better

That kid is called the **agent**

## 2ï¸âƒ£ Our game: Tic Tac Toe

The rules (simple life rules):

* 3x3 board
* Two players: âŒ and â­•
* First to align 3 in a row (horizontal, vertical, diagonal) wins
* If the board fills with no winner â†’ Draw

The agent plays against an opponent

## 3ï¸âƒ£ Who is the agent?

The **agent** is the learner

In our case:

* The agent chooses one empty square
* The environment updates the board and returns a result

Possible results:

* Win â†’ good ğŸ˜„
* Lose â†’ bad ğŸ˜¢
* Draw â†’ meh ğŸ˜
* Or game continues

## 4ï¸âƒ£ Rewards (how the agent feels)

We convert feelings into numbers (computers love numbers)

* Win  â†’ +1
* Draw â†’  0
* Lose â†’ -1
* Non-terminal move â†’ 0

This number is called the **reward**

## 5ï¸âƒ£ What is a Q-table? (the heart of RL)

Q-table = the agent's **cheat sheet / memory**

It answers this question:

ğŸ‘‰ "If I am in this board state and I place my mark in this square, how good is it?"

For Tic Tac Toe, the table is larger than Rock Paper Scissors

States = board configurations
Actions = which empty square I choose

Example (conceptual, not code):

Board state S:

âŒ â­• âŒ  
â­• âŒ â¬œ  
â¬œ â­• â¬œ  

Possible actions:

* Place at (2,0) â†’ value 0.4
* Place at (2,2) â†’ value 0.9
* Place at (1,2) â†’ value 0.1

Higher number = better idea

Simple example (monte carlo score):

<img src="qtable_ttt3.png" />

## 6ï¸âƒ£ How the Q-table is updated - QLearning Score

After each move:

1. Agent picks an action
2. Game gives a reward (if terminal)
3. Agent updates ONE number in the Q-table

Simple idea:

"Was this better or worse than I expected?"

Slow brain formula (no math panic):

new value = old value + small correction

That correction depends on reward and future hope

# Let's get technical

## 7ï¸âƒ£ Gamma (Î³) â€” thinking about the future

Gamma answers:

ğŸ‘‰ "Do I care about future rewards or only NOW?"

* Gamma close to 0

  * I only care about this move
  * Greedy mindset

* Gamma close to 1

  * I care about winning in the long run
  * Strategy brain ğŸ§ 

Typical value: **0.9**

## 8ï¸âƒ£ Epsilon (Îµ) â€” curiosity vs confidence

Epsilon answers:

ğŸ‘‰ "Should I try something random?"

* High epsilon (like 1.0)

  * Try random moves
  * Exploration phase ğŸ§ª

* Low epsilon (like 0.1)

  * Use what I already know
  * Exploitation phase ğŸ¯

Training usually looks like:

* Start with high epsilon
* Slowly reduce it

## 9ï¸âƒ£ Showing the score

We keep a score counter:

* Wins: +1
* Losses: -1
* Draws: 0

Total score shows:

ğŸ‘‰ "Is the agent actually learning?"

If score goes up over many games â†’ ğŸ‰ success

## ğŸ”Ÿ Full learning loop (slow recap)

1. Agent looks at Q-table
2. Agent maybe explores (epsilon)
3. Agent chooses a square
4. Game returns reward (if terminal)
5. Q-table is updated
6. Score is updated
7. Repeat MANY games

## Final brain-friendly summary ğŸ§ 

* Agent = learner
* Reward = feedback
* Q-table = memory
* Gamma = future thinking
* Epsilon = curiosity

Reinforcement Learning is literally:

> Try â†’ Fail â†’ Remember â†’ Improve

# Appendix A

## ğŸ§  Gamma vs Epsilon â€” why we need BOTH

### ğŸ”‘ One-line difference (lock this in)

* **Gamma (Î³)** decides **how rewards are judged** (learning)
* **Epsilon (Îµ)** decides **how actions are chosen** (behavior)

They never do the same job

## 1ï¸âƒ£ Gamma (Î³): how the agent THINKS about rewards

Gamma answers:

ğŸ‘‰ "When I update my memoryâ€¦ do I care about the future?"

* **Low gamma**

  * Only immediate reward matters
  * "Did this move win now?"

* **High gamma**

  * Immediate reward + future rewards
  * "Does this position lead to winning later?"

âš ï¸ Gamma does **NOT** choose actions

It only affects how numbers are written into the Q-table

## 2ï¸âƒ£ Epsilon (Îµ): how the agent ACTS

Epsilon answers:

ğŸ‘‰ "Do I trust my knowledge or act randomly?"

* **High epsilon**

  * Ignore the Q-table
  * Try random empty squares

* **Low epsilon**

  * Follow the highest Q-value
  * Play the best-known move

âš ï¸ Epsilon does **NOT** affect learning math

It only affects what action is picked

## 3ï¸âƒ£ The missing mental model (VERY important)

Every move has TWO separate steps:

1. **Choose an action** â†’ controlled by **epsilon**
2. **Learn from the result** â†’ controlled by **gamma**

They happen at different times

## 4ï¸âƒ£ Why low gamma does NOT replace epsilon

Key misunderstanding:

> "Low gamma means short thinking, so I don't need exploration"

Because:

* Gamma does not choose actions
* Without epsilon, the agent NEVER explores

Example:

* Agent wins once by playing center first
* Q-table prefers center
* Without epsilon â†’ agent always picks center
* Never learns better long-term patterns

## 5ï¸âƒ£ What breaks if one is missing

### âŒ No epsilon

* No exploration
* Gets stuck in habits
* Learns slowly or incorrectly

### âŒ No gamma

* No long-term planning
* Strategy never stabilizes

## ğŸ”’ Final lock-in summary

* **Gamma = how far into the future learning looks**
* **Epsilon = whether the agent explores or exploits**
* Low gamma â‰  safe choices
* Low epsilon â‰  short-term thinking

Both are required for learning to actually work
