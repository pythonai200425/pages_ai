# Reinforcement Learning (RL) â€” Rock ğŸŸ¡ Paper ğŸ“„ Scissors âœ‚ï¸

## 1ï¸âƒ£ What Reinforcement Learning even is?

Reinforcement Learning is a way to teach a computer **by experience**, not by rules

Think about a kid learning a game:

* The kid tries something
* The kid sees if it was good or bad
* The kid remembers what worked
* Next time, the kid tries to do better

That kid is called the **agent**

## 2ï¸âƒ£ Our game: Rock Paper Scissors

The rules (simple life rules):

* ğŸŸ¡ Rock beats âœ‚ï¸ Scissors
* âœ‚ï¸ Scissors beats ğŸ“„ Paper
* ğŸ“„ Paper beats ğŸŸ¡ Rock

The agent plays against an opponent

## 3ï¸âƒ£ Who is the agent?

The **agent** is the learner

In our case:

* The agent chooses one move: ğŸŸ¡ / ğŸ“„ / âœ‚ï¸
* The environment answers with a result

Possible results:

* Win â†’ good ğŸ˜„
* Lose â†’ bad ğŸ˜¢
* Draw â†’ meh ğŸ˜

## 4ï¸âƒ£ Rewards (how the agent feels)

We convert feelings into numbers (computers love numbers)

* Win  â†’ +1
* Draw â†’  0
* Lose â†’ -1

This number is called the **reward**

## 5ï¸âƒ£ What is a Q-table? (the heart of RL)

Q-table = the agent's **cheat sheet / memory**

It answers this question:

ğŸ‘‰ "If I am in this situation and I do this action, how good is it?"

For Rock Paper Scissors, the table is tiny

States = what happened last round
Actions = what I choose now

Example (conceptual, not code):

* Last round was a WIN

  * If I play ğŸŸ¡ â†’ value 0.2
  * If I play ğŸ“„ â†’ value 0.5
  * If I play âœ‚ï¸ â†’ value 0.1

Higher number = better idea

## 6ï¸âƒ£ How the Q-table is updated (baby steps)

After each round:

1. Agent picks an action
2. Game gives a reward
3. Agent updates ONE number in the Q-table

Simple idea:

"Was this better or worse than I expected?"

Slow brain formula (no math panic):

new value = old value + small correction

That correction depends on reward and future hope

## 7ï¸âƒ£ Gamma (Î³) â€” thinking about the future

Gamma answers:

ğŸ‘‰ "Do I care about future rewards or only NOW?"

* Gamma close to 0

  * I only care about this round
  * YOLO mindset

* Gamma close to 1

  * I care about winning in the long run
  * Chess brain ğŸ§ 

Typical value: **0.9**

## 8ï¸âƒ£ Epsilon (Îµ) â€” curiosity vs confidence

Epsilon answers:

ğŸ‘‰ "Should I try something random?"

* High epsilon (like 1.0)

  * Try random moves
  * Exploration phase ğŸ§ª

* Low epsilon (like 0.1)

  * Use what I already know
  * Exploitation phase ğŸ¯

Training usually looks like:

* Start with high epsilon
* Slowly reduce it

## 9ï¸âƒ£ Showing the score

We keep a score counter:

* Wins: +1
* Losses: -1
* Draws: 0

Total score shows:

ğŸ‘‰ "Is the agent actually learning?"

If score goes up over many games â†’ ğŸ‰ success

## ğŸ”Ÿ Full learning loop (slow recap)

1. Agent looks at Q-table
2. Agent maybe explores (epsilon)
3. Agent chooses ğŸŸ¡ / ğŸ“„ / âœ‚ï¸
4. Game returns reward
5. Q-table is updated
6. Score is updated
7. Repeat MANY times

## Final brain-friendly summary ğŸ§ 

* Agent = learner
* Reward = feedback
* Q-table = memory
* Gamma = future thinking
* Epsilon = curiosity

Reinforcement Learning is literally:

> Try â†’ Fail â†’ Remember â†’ Improve

# Appendix A

## ğŸ§  Gamma vs Epsilon â€” why we need BOTH 

### ğŸ”‘ One-line difference (lock this in)

* **Gamma (Î³)** decides **how rewards are judged** (learning)
* **Epsilon (Îµ)** decides **how actions are chosen** (behavior)

They never do the same job

## 1ï¸âƒ£ Gamma (Î³): how the agent THINKS about rewards

Gamma answers:

ğŸ‘‰ "When I update my memoryâ€¦ do I care about the future?"

* **Low gamma (YOLO mindset)**

  * Only immediate reward matters
  * "Did I win THIS round?"

* **High gamma (planner mindset)**

  * Immediate reward + future rewards
  * "Does this help me win MORE later?"

âš ï¸ Gamma does **NOT** choose actions

It only affects how numbers are written into the Q-table

## 2ï¸âƒ£ Epsilon (Îµ): how the agent ACTS

Epsilon answers:

ğŸ‘‰ "Do I trust my knowledge or act randomly?"

* **High epsilon**

  * Ignore the Q-table
  * Try random moves ğŸŸ¡ ğŸ“„ âœ‚ï¸

* **Low epsilon**

  * Follow the highest Q-value
  * Play the best-known move

âš ï¸ Epsilon does **NOT** affect learning math

It only affects what action is picked

## 3ï¸âƒ£ The missing mental model (VERY important)

Every round has TWO separate steps:

1. **Choose an action** â†’ controlled by **epsilon**
2. **Learn from the result** â†’ controlled by **gamma**

They happen at different times

## 4ï¸âƒ£ Why low gamma does NOT replace epsilon

This is the key misunderstanding:

> "Low gamma = YOLO, so why not just pick the safest option?"

Because:

* Gamma does not choose actions
* Without epsilon, the agent NEVER explores

Example:

* Agent once wins with ğŸŸ¡
* Q-table slightly prefers ğŸŸ¡
* Without epsilon â†’ agent plays ğŸŸ¡ forever
* Even if ğŸ“„ is better

## 5ï¸âƒ£ What breaks if one is missing

### âŒ No epsilon

* No exploration
* Gets stuck in habits
* Learns slowly or incorrectly

### âŒ No gamma

* No long-term learning
* Values stay noisy
* Strategy never stabilizes

## ğŸ”’ Final lock-in summary

* **Gamma = how far into the future learning looks**
* **Epsilon = whether the agent explores or exploits**
* Low gamma â‰  safe choices
* Low epsilon â‰  short-term thinking

Both are required for learning to actually work

