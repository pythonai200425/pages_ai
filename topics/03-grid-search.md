# ××•×¤×˜×™××™×–×¦×™×” ×©×œ ××•×“×œ×™×### GridSearchCV with KNN â€“ Explanation & Example

`GridSearchCV` from Scikit-learn helps you **find the best parameters** for your model  
by searching through all possible combinations (a "grid") of parameters.

#### ğŸ“¦ In the context of KNN:

You might want to try different values for:

- `n_neighbors`: Number of neighbors (K)
- `weights`: 
  - `'uniform'` â€” all neighbors have equal weight  
  - `'distance'` â€” closer neighbors get more weight
- `metric`: 
  - `'euclidean'` â€” standard distance  
  - `'manhattan'` â€” city block distance. It measures the distance between two points by only moving horizontally and vertically, like you would in a city with square blocks
    abs(x1-x2) + abs(y1-y2)

#### âš™ï¸ How it works:

1. You define a **grid of parameters** to test
2. `GridSearchCV` trains the model using **cross-validation** for each combination
3. It evaluates each setup using a scoring metric (e.g., accuracy)
4. It returns the **best parameter combination** based on results

#### ğŸ§  Python Example:

```python
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Example data (X, y)
# Assume X and y are already defined

# Base model
knn = KNeighborsClassifier()

# Grid of parameters to try
param_grid = {
    'n_neighbors': list(range(1, 31)),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

# Grid Search with 5-fold cross-validation
grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid.fit(X, y)

# Show best results
print("Best parameters:", grid.best_params_)
print("Best accuracy:", grid.best_score_)
```

Demo output:
```
GridSearchCV completed in 3.64 seconds
Best parameters: {'knn__n_neighbors': np.int64(11), 'knn__p': 2, 'knn__weights': 'distance'}
  p stands for the Minkowski distance formula:
  if p = 1 â†’ Manhattan (city block) distance
  if p = 2 â†’ Euclidean distance
Best accuracy: 0.8239
```



### ×›×™×¦×“ ×œ×‘×—×•×¨ ××ª K ×”××ª××™×?

1. **×’×•×“×œ ×”××“×’×**: ×›×›×œ ×©××“×’× ×”××™××•×Ÿ ×’×“×•×œ ×™×•×ª×¨, × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘-K ×’×“×•×œ ×™×•×ª×¨.
   
2. **×¨××ª ×”×¨×¢×© ×‘× ×ª×•× ×™×**: 
   - ×œ× ×ª×•× ×™× ×¢× ××¢×˜ ×¨×¢×©: K ×§×˜×Ÿ ×™×•×ª×¨ (1-5)
   - ×œ× ×ª×•× ×™× ×¢× ×”×¨×‘×” ×¨×¢×©: K ×’×“×•×œ ×™×•×ª×¨ (×œ×”×¤×—×™×ª ××ª ×”×©×¤×¢×ª ×”×¨×¢×©)

3. **××•×¨×›×‘×•×ª ×”×’×‘×•×œ×•×ª ×‘×™×Ÿ ×”××—×œ×§×•×ª**:
   - ×’×‘×•×œ×•×ª ×¤×©×•×˜×™×: K ×’×“×•×œ ×™×•×ª×¨
   - ×’×‘×•×œ×•×ª ××•×¨×›×‘×™×: K ×§×˜×Ÿ ×™×•×ª×¨

4. **×›×œ×œ ××¦×‘×¢**: ×œ×¢×ª×™× ×§×¨×•×‘×•×ª ××•××œ×¥ ×œ×”×ª×—×™×œ ×¢× $K = \sqrt{n}$ ×›××©×¨ n ×”×•× ××¡×¤×¨ ×”×“×•×’×××•×ª ×‘××“×’× ×”××™××•×Ÿ.

5. **×¢×¨×›×™× ××™-×–×•×’×™×™×**: ×¢×‘×•×¨ ×‘×¢×™×•×ª ×¡×™×•×•×’ ×‘×™× ××¨×™, ×›×“××™ ×œ×‘×—×•×¨ ×¢×¨×›×™ K ××™-×–×•×’×™×™× ×›×“×™ ×œ×× ×•×¢ ×ª×™×§×•.

×‘×—×™×¨×ª ×¢×¨×š K ×”××•×¤×˜×™××œ×™ ×”×™× ××¤×ª×— ×œ×”×¦×œ×—×ª ××œ×’×•×¨×™×ª× KNN:

**×©×™×˜×•×ª ××•××œ×¦×•×ª ×œ×‘×—×™×¨×ª K**:
   - ×¢×‘×•×¨ ××“×’××™× ×§×˜× ×™×: ××™××•×ª ×¦×•×œ×‘ (cross-validation)
   - ×¢×‘×•×¨ ××“×’××™× ×’×“×•×œ×™×: ×©×™×˜×ª Elbow ××• GridSearchCV
   - ×©×™××•×© ×‘-K ××™-×–×•×’×™ ×œ×× ×™×¢×ª ×ª×™×§×• ×‘×‘×¢×™×•×ª ×‘×™× ××¨×™×•×ª

**×˜×™×¤×•×œ ×‘××§×¨×™ ×ª×™×§×•**:
   - ×©×™××•×© ×‘××©×§×•×œ×•×ª ××¨×—×§ (`weights='distance'`)
   - ×‘×—×™×¨×ª K ××™-×–×•×’×™
   - ×”×¤×—×ª×ª K ××• ×”×’×“×œ×ª×• ×‘××§×¨×” ×”×¦×•×¨×š